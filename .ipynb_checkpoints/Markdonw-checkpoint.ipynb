{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ad3136-ab0a-46d4-a03d-9072c7050fe1",
   "metadata": {},
   "source": [
    "**2. Objetivos**\n",
    "   \n",
    "2.1. Objetivo General\n",
    "Diseñar, desarrollar y documentar una herramienta de código abierto que, mediante la aplicación de técnicas de Procesamiento de Lenguaje Natural, permita un análisis cuantitativo y cualitativo avanzado de los resultados de encuestas Delphi a gran escala.\n",
    "\n",
    "2.2. Objetivos Específicos\n",
    "\n",
    "a) Evaluar la estructura y complejidad del conjunto de datos resultante de la encuesta Delphi del INTA (2012) como caso de estudio representativo. \n",
    "\n",
    "b) Definir el alcance funcional de la herramienta, estableciendo las variables y métricas clave a generar a partir de los datos originales.\n",
    "\n",
    "c) Desarrollar un prototipo funcional en un entorno reproducible (notebook de Python), definiendo el lenguaje y las librerías especializadas a utilizar.\n",
    "\n",
    "d) Implementar una metodología para la explotación de los resultados, incluyendo la creación de un score de calidad argumental y la segmentación del panel de expertos. \n",
    "\n",
    "e) Documentar el proceso de desarrollo, detallando las decisiones metodológicas y los desafíos técnicos encontrados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0bffc3e-0c1b-414f-8496-7475d84d20d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '—' (U+2014) (3861157541.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mEl nombre \"Delphi\" evoca deliberadamente al antiguo Oráculo de Delfos, donde los sacerdotes interpretaban las palabras a menudo ambiguas de la Pitia para predecir el futuro. Esta elección de nombre, sin embargo, es profundamente irónica. El método Delphi moderno, desarrollado en la década de 1950, representa precisamente el esfuerzo por reemplazar la ambigüedad profética con un procedimiento sistemático, transparente y reproducible para la exploración del futuro. Su génesis en la Corporación RAND, el think tank por excelencia de la era de la Guerra Fría, no fue accidental. En un contexto de alta incertidumbre tecnológica y geopolítica, donde el juicio de un solo individuo —sin importar cuán experto fuera— resultaba insuficiente y riesgoso, se necesitaba con urgencia una metodología para agregar el conocimiento disperso y a menudo tácito de múltiples especialistas.\u001b[39m\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '—' (U+2014)\n"
     ]
    }
   ],
   "source": [
    "**Marco Teórico: Fundamentos para un Análisis Aumentado de la Prospectiva**\n",
    "    \n",
    "Introducción al Marco Teórico\n",
    "El presente trabajo se sitúa en la intersección de dos disciplinas en aparente disimilitud: la prospectiva estratégica, un campo eminentemente cualitativo y deliberativo, y la ciencia de datos, con su enfoque cuantitativo y algorítmico. El objetivo de este capítulo es establecer los cimientos teóricos que no solo justifican esta convergencia, sino que demuestran su potencial para enriquecer radicalmente el análisis de futuros. Para poder valorar la herramienta de Procesamiento de Lenguaje Natural (NLP) desarrollada en las páginas siguientes, es imprescindible comprender primero la naturaleza, las complejidades, las fortalezas y las limitaciones de su principal objeto de estudio: los datos generados por una encuesta Delphi.\n",
    "La narrativa de este capítulo está diseñada para guiar al lector en un recorrido lógico y progresivo. Se iniciará con una profunda revisión del método Delphi, explorando no solo sus fundamentos operativos sino también su contexto histórico y sus debates epistemológicos. Se continuará con la contextualización detallada del caso de estudio, la encuesta del INTA, presentándola no como un mero conjunto de datos, sino como un artefacto complejo representativo de los desafíos del \"Big Data cualitativo\". Finalmente, se mapeará el panorama tecnológico, desde las herramientas de software tradicionales hasta la frontera de la inteligencia artificial, para identificar con precisión el vacío de conocimiento y la oportunidad metodológica que este trabajo final busca abordar. Este marco teórico, por lo tanto, no es un mero requisito formal, sino el argumento central que da sentido y urgencia a la solución propuesta.\n",
    "1. El Método Delphi: Orquestando el Consenso y Disenso Experto\n",
    "1.1. Orígenes, Contexto y Fundamentos Filosóficos: De la Profecía a la Metodología\n",
    "El nombre \"Delphi\" evoca deliberadamente al antiguo Oráculo de Delfos, donde los sacerdotes interpretaban las palabras a menudo ambiguas de la Pitia para predecir el futuro. Esta elección de nombre, sin embargo, es profundamente irónica. El método Delphi moderno, desarrollado en la década de 1950, representa precisamente el esfuerzo por reemplazar la ambigüedad profética con un procedimiento sistemático, transparente y reproducible para la exploración del futuro. Su génesis en la Corporación RAND, el think tank por excelencia de la era de la Guerra Fría, no fue accidental. En un contexto de alta incertidumbre tecnológica y geopolítica, donde el juicio de un solo individuo —sin importar cuán experto fuera— resultaba insuficiente y riesgoso, se necesitaba con urgencia una metodología para agregar el conocimiento disperso y a menudo tácito de múltiples especialistas.\n",
    "El proyecto, liderado por los matemáticos Olaf Helmer y Norman Dalkey, junto con el filósofo Nicholas Rescher, partió de una crítica fundamental a los métodos tradicionales de toma de decisiones en grupo: los comités. Las reuniones cara a cara, argumentaban, estaban plagadas de \"ruido\" comunicacional que distorsionaba la \"señal\" del conocimiento genuino. Factores como la elocuencia de un orador, la reputación de un experto, la presión por la conformidad grupal o las dinámicas de poder jerárquico podían llevar a un consenso prematuro o erróneo. El Delphi fue concebido, por tanto, como un \"procedimiento para 'la cristalización de la opinión de un grupo' a través de una serie de cuestionarios individuales intercalados con retroalimentación de la opinión del grupo\" (Dalkey & Helmer, 1963). Su filosofía subyacente es que un juicio grupal anónimo e iterativo es superior tanto a la opinión individual como a las decisiones de un comité tradicional.\n",
    "Epistemológicamente, el Delphi se sitúa en un interesante punto medio. Si bien sus orígenes son claramente positivistas —buscando objetividad y predicción a través de la agregación estadística—, su práctica ha evolucionado para abrazar una perspectiva más interpretativista. Se reconoce que, en problemas complejos (\"wicked problems\"), no existe una única verdad objetiva por descubrir. En cambio, el conocimiento es una construcción social, y el valor del Delphi reside en su capacidad para mapear el paisaje de las subjetividades expertas, estructurar el debate y facilitar un aprendizaje colectivo que conduzca a una comprensión más rica y matizada del problema (Hasson, Keeney, & McKenna, 2000).\n",
    "Los pilares conceptuales del método, que han sido validados y refinados a lo largo de más de medio siglo de aplicación en campos tan diversos como la salud, la educación, la tecnología y la política pública, son la clave de su perdurable relevancia:\n",
    "•\tAnonimato y Desvinculación Psicológica: El anonimato es la condición sine qua non del método. Al disociar la opinión de la identidad del experto, se neutralizan sesgos cognitivos profundamente arraigados en la interacción humana. Se mitiga el efecto de arrastre (bandwagon effect), donde los individuos tienden a adoptar la opinión mayoritaria por un deseo de aceptación social, y el efecto halo, que otorga un peso desproporcionado a la opinión de figuras de alta reputación o carisma, independientemente del fundamento de sus argumentos. El anonimato fomenta la honestidad intelectual, protege a los participantes con opiniones minoritarias o controvertidas y, crucialmente, obliga a que las ideas sean evaluadas por su mérito intrínseco y la fuerza del argumento que las respalda, no por su origen.\n",
    "•\tIteración y Aprendizaje Colectivo: Un estudio Delphi no es una fotografía estática, sino una película que captura la evolución del pensamiento de un grupo. Se desarrolla en rondas sucesivas donde el panel de expertos tiene la oportunidad de refinar sus juicios. La primera ronda suele ser una tormenta de ideas abierta para definir el alcance del problema, mientras que las rondas subsiguientes se centran en evaluar, priorizar y converger sobre los temas identificados. Este carácter iterativo, como señalan Linstone y Turoff, transforma una simple consulta en una \"comunicación grupal a distancia\" (Linstone & Turoff, 1975). Es un proceso de aprendizaje donde el colectivo reflexiona sobre sí mismo, considerando nuevas perspectivas y ajustando sus posiciones iniciales a la luz de los argumentos y la distribución de las opiniones del resto del panel.\n",
    "•\tRetroalimentación Controlada (Feedback): Este es el motor del proceso iterativo y la tarea más crítica del equipo coordinador o \"monitor\". Entre rondas, este equipo actúa como un centro de inteligencia imparcial. Su labor no es meramente administrativa, sino profundamente analítica. Deben procesar un gran volumen de respuestas, sintetizarlas sin introducir sesgos y devolverlas al panel de una manera que sea informativa y estimule la reflexión. Esta retroalimentación es un artefacto analítico que típicamente incluye:\n",
    "o\tPara las preguntas cuantitativas: Medidas de tendencia central (casi siempre la mediana, por ser menos sensible a valores extremos que la media) y medidas de dispersión (usualmente el rango intercuartílico, que muestra el rango del 50% central de las respuestas). La presentación de estos datos permite a cada experto autoevaluar su posición: \"¿Estoy dentro del consenso? ¿Soy un caso atípico? ¿Por qué?\".\n",
    "o\tPara las justificaciones cualitativas: La síntesis de los argumentos es la parte más artesanal y desafiante. El equipo debe identificar, agrupar y resumir las principales razones esgrimidas a favor y en contra de una determinada postura. Es vital que se dé igual prominencia a los argumentos que sustentan las opiniones minoritarias, ya que a menudo contienen las semillas de futuros cambios de paradigma o señalan riesgos no considerados por la mayoría. La calidad de esta síntesis es directamente proporcional a la calidad de la reflexión en la ronda siguiente.\n",
    "•\tRespuesta Estadística del Grupo y la Valoración del Disenso: Contrario a una concepción errónea y bastante extendida, el objetivo final del Delphi no es forzar un consenso total. Es, más bien, generar una fotografía lo más nítida posible del estado del juicio experto sobre un tema, incluyendo sus certidumbres e incertidumbres. El resultado es una \"respuesta de grupo\" que refleja tanto el punto de mayor convergencia (la mediana) como el grado de disenso (la dispersión). Como argumenta con agudeza Landeta, la validez y utilidad del método residen precisamente en su capacidad para hacer aflorar y estructurar el desacuerdo (Landeta, 2006). Un alto grado de disenso no es un fracaso del método, sino un hallazgo en sí mismo: indica que el futuro es altamente incierto, que existen escuelas de pensamiento contrapuestas o que la evidencia disponible es ambigua. Identificar las razones subyacentes a este disenso es, a menudo, el resultado más valioso de un estudio Delphi.\n",
    "1.2. El Rol del Delphi en la Prospectiva Estratégica: Un Instrumento de Exploración y Construcción de Futuros\n",
    "Dentro del vasto conjunto de herramientas de la prospectiva estratégica y los estudios de futuros (futures studies), el método Delphi ocupa un lugar privilegiado por su versatilidad. Su aplicación en este campo trasciende la simple previsión (forecasting), que se enfoca en determinar el futuro más probable, a menudo extrapolando tendencias pasadas. La prospectiva (foresight), en cambio, tiene una ambición mayor: iluminar el abanico de futuros posibles, plausibles y deseables para poder influir en su construcción. En este contexto, el Delphi se convierte en una herramienta exploratoria, normativa y constructivista.\n",
    "Se pueden distinguir diversas variantes del método según su finalidad estratégica:\n",
    "•\tDelphi Exploratorio o Convencional: Es la aplicación más clásica. Busca identificar tendencias emergentes (señales débiles), incertidumbres críticas, factores de cambio y posibles eventos disruptivos (cisnes negros o cisnes grises). Su objetivo es expandir la visión de los tomadores de decisiones, ayudándolos a \"pensar lo impensable\" y a prepararse para un rango más amplio de contingencias.\n",
    "•\tDelphi Normativo o de Políticas (Policy Delphi): Esta variante, desarrollada por Murray Turoff, no busca predecir el futuro, sino forjarlo. Se enfoca en evaluar la viabilidad, deseabilidad, aceptabilidad e impacto potencial de diferentes opciones de política, estrategias o cursos de acción. Su objetivo es generar un debate estructurado entre diferentes stakeholders o grupos de interés para encontrar puntos de acuerdo, identificar conflictos de valores irreconciliables y, en última instancia, construir consenso sobre las acciones a tomar para alcanzar un futuro deseado. A menudo, las preguntas no son sobre lo que será, sino sobre lo que debería ser.\n",
    "•\tDelphi Argumentativo (Argument Delphi): Una variante más reciente que pone aún más énfasis en la parte cualitativa. El foco principal no está en la convergencia numérica, sino en la construcción y el mapeo de los árboles argumentales. Se busca identificar todas las posibles posturas sobre un tema y, más importante aún, todas las evidencias, supuestos y líneas de razonamiento que las sustentan. El resultado es un mapa detallado de la controversia, que es invaluable para entender la estructura de debates complejos.\n",
    "Independientemente de su variante, el Delphi es un pilar para la construcción de escenarios, quizás la técnica de prospectiva más conocida. Permite identificar las \"fuerzas motrices\" y las \"incertidumbres críticas\" que se utilizarán como ejes para definir las lógicas de los diferentes escenarios futuros.\n",
    "Pese a su potencia, el método enfrenta desafíos metodológicos persistentes que son centrales para la justificación de este trabajo. La selección del panel de expertos es el factor más crítico; un panel sesgado producirá resultados sesgados. La fatiga y el abandono de los participantes a lo largo de las rondas (panel attrition) pueden comprometer la validez de los resultados. Y, como se ha insistido, persiste la tensión analítica entre lo cuantitativo y lo cualitativo. El análisis tradicional, al tratar por separado el \"qué\" (el voto numérico) y el \"porqué\" (la justificación textual), deja sin explotar la sinergia más rica de los datos. Esta brecha metodológica, reconocida en la literatura especializada (Okoli & Pawlowski, 2004), es el terreno fértil donde la ciencia de datos puede ofrecer un aporte transformador, pasando de un análisis meramente descriptivo a uno verdaderamente explicativo e interpretativo.\n",
    "2. El Caso de Estudio: La Encuesta INTA 2012 y el Desafío del \"Big Data Cualitativo\"\n",
    "Si el método Delphi es el marco conceptual, el conjunto de datos de la encuesta del INTA de 2012 es el terreno empírico sobre el cual se construye y valida este trabajo. Este estudio no fue un ejercicio académico aislado, sino una iniciativa de gran envergadura enmarcada en el \"Proyecto MINCyT-BIRF: Estudios del Sector Agroindustria\", un esfuerzo estratégico por diagnosticar y explorar el futuro de uno de los motores económicos y sociales de la Argentina. Para comprender la magnitud del desafío analítico y la necesidad de una nueva aproximación metodológica, es indispensable realizar una disección profunda de este artefacto de datos, desde su estructura y contenido hasta las limitaciones inherentes a su análisis tradicional.\n",
    "2.1. Anatomía de un Instrumento Prospectivo Complejo: La tablabase_plana2.csv\n",
    "El archivo tablabase_plana2.csv es más que un simple set de datos; es la materialización digital de un diálogo estructurado con más de doscientos de los más destacados expertos del sector agroindustrial argentino de la época. Su aparente simplicidad como tabla oculta una densidad y multidimensionalidad que merecen ser desglosadas para apreciar la verdadera naturaleza del problema.\n",
    "•\tAlcance y Granularidad Temática: La encuesta se diseñó para capturar una visión holística del sector, abarcando toda la cadena de valor y su contexto. Los \"Bloques temáticos\" (Bloque temático en la tabla) son la primera evidencia de esta ambición, cubriendo desde \"Incertidumbres críticas sobre la Agroindustria Alimentaria Argentina, en el contexto mundial\" hasta cadenas de valor específicas como la \"Cadena cárnica\", \"Cadena láctea\", \"Frutas finas\", \"Sector porcino\", entre otras. Esta estructura implica que el estudio no fue una única encuesta, sino, en la práctica, un conjunto de sub-estudios Delphi paralelos, cada uno con su propio panel de especialistas, unificados bajo un marco común.\n",
    "•\tLa Unidad de Análisis: Una Respuesta Multidimensional: La estructura \"larga\" del archivo, donde cada fila (pk_id) representa la respuesta de un experto a un ítem específico, revela la complejidad de la unidad de análisis. Cada respuesta no es un simple voto, sino un vector de información que combina:\n",
    "o\tIdentificadores: email, row_id, col_id, que permiten rastrear la respuesta hasta el experto y el ítem específico del cuestionario.\n",
    "o\tValoraciones Cuantitativas Múltiples: Las columnas int_ans1 a int_ans6 capturan diferentes dimensiones numéricas de la opinión del experto. Típicamente, estas correspondían a la probabilidad de ocurrencia, el nivel de impacto (a menudo en una escala de alto impacto positivo a alto impacto negativo), y el horizonte temporal estimado para la consolidación del evento o tendencia descrita en el enunciado. Esta riqueza cuantitativa ya presenta un desafío: un evento puede ser considerado de bajo impacto pero alta probabilidad, o viceversa, y un análisis que solo promedie una de estas dimensiones pierde información crucial.\n",
    "o\tEl Corazón Cualitativo: El campo reason (razón o justificación) es, sin duda, el más valioso y, a la vez, el más problemático desde una perspectiva de análisis tradicional. Este campo de texto libre era el espacio para que el experto argumentara su voto, proveyera contexto, señalara condiciones, expresara dudas o matices. Es la diferencia entre saber qué votó el panel y empezar a entender por qué lo hizo.\n",
    "•\tLa Diversidad del Panel de Expertos: Las columnas employment sector, level of education, function y years exper nos permiten caracterizar el panel. No era un grupo homogéneo, sino un mosaico de perfiles que incluía actores del sector de \"Ciencia y Tecnología\", \"Sector Público\", \"Sector Privado\" y \"Universidad\". Sus funciones iban desde \"Director/Gerente\" hasta \"Consultor/Asesor\" o \"Investigador\". Esta heterogeneidad es una fortaleza del estudio, pero también una variable de complejidad analítica. ¿Existen diferencias sistemáticas en la forma de argumentar entre un científico y un empresario? ¿Los expertos con más años de experiencia (years exper) ponderan los riesgos de manera diferente? Estas son preguntas estratégicas que el análisis manual solo puede abordar de forma anecdótica.\n",
    "2.2. El Reto del Análisis: \"Big Qualitative Data\" y la Fatiga del Analista\n",
    "Los informes cualitativo y cuantitativo generados en 2012 son el producto de un análisis riguroso bajo el paradigma metodológico de su tiempo. El informe cuantitativo agrega las respuestas numéricas, mientras que el informe cualitativo representa un esfuerzo heroico de síntesis por parte del equipo coordinador. Sin embargo, este enfoque manual se enfrenta a dos barreras fundamentales cuando se aplica a un volumen de datos de esta magnitud, barreras que la tecnología actual nos permite nombrar y abordar: el \"Big Qualitative Data\" y la \"Fatiga del Analista\".\n",
    "•\tEl \"Big Qualitative Data\": Este término, aunque no de uso extendido, describe perfectamente el desafío. A diferencia del Big Data tradicional, definido por las \"V\" de Volumen, Velocidad y Variedad en datos estructurados o semi-estructurados, el \"Big Qualitative Data\" se refiere a conjuntos de datos cuyo principal desafío es el Volumen de texto no estructurado. En el caso del INTA, hablamos de miles de justificaciones textuales. La variedad no está en los formatos, sino en la riqueza semántica: diversidad de estilos de escritura, terminología específica de cada sub-sector, y complejidad variable de los argumentos. La velocidad se refiere a la necesidad de procesar esta información en un tiempo razonable para que los resultados del estudio prospectivo no se vuelvan obsoletos. El análisis de este tipo de datos con métodos manuales (lectura, codificación y categorización humana) se vuelve exponencialmente más lento y costoso a medida que el volumen crece, y lo que es más importante, se vuelve más propenso a errores e inconsistencias.\n",
    "•\tLa Fatiga y el Sesgo del Analista: La segunda barrera es de naturaleza cognitiva. Incluso el equipo de analistas más experimentado y disciplinado es susceptible a la fatiga y a sesgos inherentes al procesamiento de información cualitativa a gran escala (Krippendorff, 2018). Cuando se enfrenta a la tarea de leer y sintetizar la justificación número 5.000, la capacidad de un analista para detectar matices sutiles, mantener la consistencia en la codificación y evitar el sesgo de confirmación (prestar más atención a los argumentos que confirman sus hipótesis previas) disminuye inevitablemente. Se corre el riesgo de sobrerrepresentar los argumentos mejor redactados o los que aparecen con mayor frecuencia, mientras que las \"señales débiles\" —argumentos minoritarios, pero potencialmente visionarios— pueden perderse en el ruido. La síntesis narrativa final, aunque valiosa, es necesariamente una reducción de la complejidad original, una única interpretación posible de entre muchas.\n",
    "La herramienta de NLP propuesta en este trabajo no busca criticar el análisis original, sino reconocer estas limitaciones inherentes y proponer una evolución metodológica. No se trata de reemplazar al analista humano, sino de empoderarlo. La herramienta actúa como un asistente incansable y sistemático, capaz de realizar una primera lectura completa de todos los textos, cuantificar aspectos como su calidad argumental de forma objetiva, y presentar los datos de una manera que permita al analista humano enfocar su atención donde más valor puede aportar: en la interpretación estratégica de los patrones y anomalías revelados por la máquina.\n",
    "2.3. La tablabase_plana2.csv como Artefacto de Inteligencia Colectiva\n",
    "Es útil, finalmente, conceptualizar el conjunto de datos del INTA no solo como un insumo para el análisis, sino como un artefacto de inteligencia colectiva. Es una cápsula del tiempo digital que congela el conocimiento, las expectativas, los temores y las esperanzas del sector agroindustrial argentino en un momento específico de su historia. Contiene las huellas de debates, consensos y disensos que definieron la agenda de innovación de la década siguiente.\n",
    "Sin embargo, gran parte del valor de este artefacto ha permanecido latente, atrapado en la masividad de sus datos no estructurados. Las herramientas analíticas de 2012 permitieron extraer una parte significativa de su valor, pero no todo. La promesa de la ciencia de datos y el NLP es la de construir una nueva \"llave\" capaz de abrir las cámaras más profundas de este archivo.\n",
    "El presente trabajo, por lo tanto, se enmarca en una suerte de \"arqueología de datos\". Busca aplicar técnicas computacionales avanzadas a un conjunto de datos histórico para revelar patrones que no eran visibles con las herramientas de su época. El desarrollo de los \"arquetipos de expertos\" es el ejemplo más claro de este enfoque: es una estructura que no existía explícitamente en los datos, sino que emerge de ellos cuando se los ilumina con la luz del análisis de clusters y el scoring de calidad textual. En última instancia, la herramienta desarrollada no es solo un procesador de encuestas, sino un instrumento para descifrar y reactivar la inteligencia colectiva contenida en artefactos de datos complejos como el de la encuesta del INTA.\n",
    "3. La Tecnología como Puente: Del Software de Encuestas al Análisis Semántico\n",
    "La práctica de la prospectiva, y del método Delphi en particular, nunca ha existido en un vacío tecnológico. Por el contrario, sus capacidades y limitaciones han estado históricamente ligadas a las herramientas disponibles para la comunicación y el análisis. La evolución desde los cuestionarios enviados por correo postal hasta las plataformas de inteligencia artificial no solo ha representado un cambio en eficiencia, sino una transformación fundamental en lo que es posible analizar y comprender. Este apartado traza dicha evolución para contextualizar la emergencia del Procesamiento de Lenguaje Natural (NLP) como la frontera actual y para posicionar la herramienta desarrollada en este trabajo como una contribución lógica y necesaria a esta trayectoria.\n",
    "3.1. Una Breve Historia del Software para Delphi: De la Gestión a la Interpretación\n",
    "La instrumentación tecnológica del método Delphi puede clasificarse en tres generaciones, cada una representando un salto cualitativo en la capacidad de gestionar la complejidad del proceso.\n",
    "1.\tGeneración 1: La Era de la Digitalización (Plataformas de Encuestas Genéricas). La primera ola tecnológica consistió en la adopción de herramientas de encuestas en línea (ej. SurveyMonkey, LimeSurvey, Google Forms) para reemplazar los procesos manuales basados en papel y correo. Este paso, aunque aparentemente simple, significó una mejora radical en la logística: redujo drásticamente los tiempos de distribución y recolección, simplificó la tabulación de datos cuantitativos y facilitó la comunicación con paneles de expertos geográficamente dispersos. Sin embargo, estas herramientas eran agnósticas al método Delphi. El andamiaje analítico esencial —el cálculo de medianas y rangos intercuartílicos, la preparación de los informes de retroalimentación y, sobre todo, la ardua tarea de sintetizar los argumentos cualitativos— debía realizarse de forma completamente externa, exportando los datos a hojas de cálculo y procesadores de texto. La tecnología era un asistente administrativo, no un socio analítico.\n",
    "2.\tGeneración 2: La Era de la Automatización (Sistemas de Gestión Delphi Dedicados). Reconociendo las limitaciones de las herramientas genéricas, la segunda generación vio el desarrollo de plataformas de software diseñadas específicamente para el flujo de trabajo Delphi. Estos sistemas integran en un solo entorno la gestión del panel, la administración de rondas, el cálculo automático de estadísticas de consenso y la generación de informes de retroalimentación personalizados para cada experto. Representaron un gran avance en la reducción de la carga de trabajo manual y en la estandarización del proceso, haciéndolo más riguroso y reproducible. No obstante, su capacidad analítica sobre el texto cualitativo permaneció, en su mayor parte, rudimentaria. A menudo se limitaba a funcionalidades descriptivas como nubes de palabras (que destacan los términos más frecuentes pero ignoran el contexto) o análisis de frecuencia, que, si bien son útiles, apenas rozan la superficie del significado semántico. La interpretación profunda del \"porqué\" seguía siendo una tarea exclusivamente humana.\n",
    "3.\tGeneración 3: La Era del Análisis Aumentado (Integración de Inteligencia Artificial). La generación actual y futura se define por la integración de técnicas de Inteligencia Artificial, y en particular de NLP, directamente en el ciclo de análisis. El objetivo ya no es solo gestionar y automatizar el proceso, sino \"aumentar\" la capacidad cognitiva del analista humano. Se trata de construir herramientas que actúen como un socio analítico, capaces de leer, comprender y estructurar el vasto corpus de texto generado en un Delphi, permitiendo al analista enfocarse en la interpretación estratégica de alto nivel. La herramienta desarrollada en este trabajo es un prototipo representativo de esta tercera generación.\n",
    "3.2. Introducción al Procesamiento de Lenguaje Natural (NLP) para el Análisis Prospectivo\n",
    "Para comprender cómo una máquina puede \"leer\" y \"cuantificar\" la calidad de un argumento, es necesario desmitificar el NLP y entenderlo como un conjunto de técnicas computacionales aplicadas. El NLP es una rama de la inteligencia artificial que se ocupa de la interacción entre las computadoras y el lenguaje humano. Para los fines de este trabajo, no es necesario ser un experto en algoritmos, pero sí comprender la lógica detrás de los métodos empleados para transformar el texto de la columna reason en las métricas y arquetipos que se presentan.\n",
    "•\tFundamentos: Pre-procesamiento y Vectorización. Antes de cualquier análisis, el texto crudo debe ser preparado en un proceso análogo a la limpieza de cualquier otro tipo de datos. Esto incluye la tokenización (dividir el texto en unidades, como palabras), la conversión a minúsculas, la eliminación de stop words (palabras comunes como \"el\", \"que\", \"y\", que actúan como ruido) y la lematización o stemming (reducir las palabras a su raíz para que \"argumentó\", \"argumentando\" y \"argumentos\" se traten como la misma unidad conceptual: \"argumentar\"). Una vez limpio, el texto debe ser convertido a un formato numérico que los algoritmos puedan procesar, un paso conocido como vectorización. Métodos como TF-IDF (Term Frequency-Inverse Document Frequency) son clave aquí. TF-IDF no solo cuenta la frecuencia de una palabra en un texto, sino que pondera esa frecuencia por su rareza en el conjunto de todos los textos. Así, una palabra como \"soja\" puede ser frecuente en una respuesta, pero si aparece en muchas respuestas sobre la agroindustria, su peso se atenúa; mientras que una palabra más específica como \"biotecnología\" o \"trazabilidad\" recibirá un peso mayor si es menos común en el corpus general, señalando su importancia distintiva en el argumento que la contiene (Ignatow & Mihalcea, 2016).\n",
    "•\tIngeniería de Características Textuales: Proxies de Calidad. La herramienta desarrollada en este trabajo hace un uso inteligente de la \"ingeniería de características\", que consiste en crear nuevas variables a partir de los datos existentes. Las métricas como largo_frase y palabras_unicas son ejemplos perfectos. No miden directamente la \"calidad\" de un argumento —un concepto semántico complejo—, sino que calculan proxies o indicadores indirectos y cuantificables de su complejidad y riqueza. La hipótesis subyacente, que luego se valida empíricamente, es que los argumentos más sólidos tienden a ser más elaborados (mayor longitud) y a utilizar un vocabulario más rico y preciso (más palabras únicas). La creación del score_calidad combina estas métricas en un único indicador, transformando una evaluación subjetiva en un ranking objetivo y reproducible.\n",
    "•\tAprendizaje No Supervisado: Descubriendo Estructuras Latentes. La creación de los \"arquetipos de expertos\" es el resultado de aplicar técnicas de aprendizaje no supervisado, específicamente el análisis de clusters. Estos algoritmos, como K-Means, agrupan los datos (en este caso, a los expertos) en función de su similitud a través de las variables seleccionadas (como el score_calidad y la participación). El algoritmo no sabe de antemano qué son un \"Experto Ancla\" o un \"Votante\"; simplemente encuentra grupos matemáticamente coherentes en los datos. La tarea del analista humano es, posteriormente, interpretar estos clusters y darles un nombre y un significado estratégico. Este es un ejemplo perfecto de la simbiosis hombre-máquina: la máquina identifica los patrones a una escala imposible para un humano, y el humano les otorga sentido y relevancia contextual.\n",
    "3.3. Experiencias Internacionales, el Nicho Metodológico y la Contribución Original\n",
    "La aplicación de la ciencia de datos a la prospectiva no es una novedad absoluta. A nivel internacional, organizaciones como la OCDE, la Comisión Europea o el Foro Económico Mundial emplean rutinariamente técnicas de horizon scanning automatizado. Utilizan algoritmos para analizar millones de artículos de noticias, publicaciones científicas y patentes para detectar \"señales débiles\" de cambio. Aplican análisis de redes sociales para entender la difusión de narrativas sobre nuevas tecnologías o análisis de sentimiento para medir la percepción pública.\n",
    "Sin embargo, una revisión de la literatura y la práctica revela una distinción crucial. La gran mayoría de estas aplicaciones de IA están orientadas \"hacia afuera\": analizan datos masivos y exógenos para informar el proceso prospectivo. Su objetivo es traer el mundo exterior al taller del futurista.\n",
    "La contribución original y el nicho metodológico de este trabajo final residen en su enfoque \"hacia adentro\". La herramienta aquí desarrollada no mira al mundo exterior, sino que utiliza el poder del NLP para analizar la dinámica interna y la riqueza argumental del propio panel de expertos Delphi. No busca encontrar tendencias en Twitter, sino entender la estructura del pensamiento del colectivo de especialistas convocados.\n",
    "Este es un vacío significativo en la literatura y en la caja de herramientas del prospectivista. Mientras que se ha escrito mucho sobre cómo seleccionar expertos, la pregunta de cómo evaluar diferencialmente la calidad de su contribución durante el análisis ha permanecido largamente sin respuesta metodológica rigurosa. La ponderación implícita siempre ha sido \"un experto, un voto\".\n",
    "Este trabajo desafía ese supuesto. Al crear un score_calidad, se introduce la posibilidad de un análisis ponderado, donde las opiniones respaldadas por argumentos más sólidos pueden recibir una mayor atención. Al definir los \"arquetipos\", se pasa de una visión monolítica del \"panel de expertos\" a una comprensión segmentada y matizada de los diferentes roles que los participantes juegan en la construcción del conocimiento colectivo. Permite responder preguntas que antes eran inaccesibles: ¿El consenso está impulsado por los \"Votantes\" o por los \"Expertos Ancla\"? ¿Las opiniones más divergentes y minoritarias provienen de especialistas con argumentos sólidos?\n",
    "En conclusión, la herramienta propuesta no es simplemente una aplicación de una nueva tecnología a un viejo método. Representa un avance conceptual: el paso de un análisis de consenso a un análisis de la arquitectura del consenso. Busca transformar el producto final de un Delphi de una simple lista de resultados estadísticos a un mapa rico y detallado del paisaje intelectual del panel de expertos. Este marco teórico, desde los fundamentos filosóficos del Delphi hasta la vanguardia del NLP, ha buscado establecer la lógica, la necesidad y la validez de esta ambiciosa y necesaria evolución.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54818d-7028-4240-a636-b6b2ddee79f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
